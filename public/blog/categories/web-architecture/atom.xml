<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Web Architecture | The Technician]]></title>
  <link href="http://chr.ishenry.com/blog/categories/web-architecture/atom.xml" rel="self"/>
  <link href="http://chr.ishenry.com/"/>
  <updated>2013-06-30T22:01:36-04:00</updated>
  <id>http://chr.ishenry.com/</id>
  <author>
    <name><![CDATA[Chris Henry]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Search is Hard]]></title>
    <link href="http://chr.ishenry.com/2010/10/09/search-is-hard/"/>
    <updated>2010-10-09T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2010/10/09/search-is-hard</id>
    <content type="html"><![CDATA[<p>The title of this post is a direct quote from a Facebook engineer presenting
at the SXSW panel <a href="http://my.sxsw.com/events/event/386">Beyond Lamp</a>. Search
is a critical function of any site, but its gotten much much harder as Google
has gotten better. To quote the Beyond Lamp panel one more time:</p>

<blockquote><p>Search is always compared against Google, which is like comparing the canoe
you just built to the QE2.</p></blockquote>

<p>The difficulty of search is made apparent by the majority of sites, even major
sites get it wrong. A large factor in the success of search is relevancy.
Google takes into account <a href="http://en.wikipedia.org/wiki/PageRank">500 million
variables</a> in determining how relevant
content is. Not only that, but they also know who you are, what you've
clicked, and can make decisions based on that to present pages that are more
relevant to you. Facebook's EdgeRank, LinkedIn's Signal are other examples of
search implementations that are vast in scale.</p>

<p>In a startup, where time is of the essence and resources need to be begged,
borrowed or stolen, search is a huge challenge. Like trying to be build the
QE2 with nothing but a swiss army knife. Basic tools normally don't cut it.
MySQL's FULLTEXT indexes are helpful, but start trying to implement basic IR
techniques like booleans, and MySQL's builtin functionality starts to lack the
ability to get the results your want.</p>

<p>There are ways to simplify building search.
<a href="http://chr.ishenry.com/2010/01/28/sphinx-full-text-search-engine/">Sphinx</a>
provides great matching capabilities and incredibly fast sorting. When
combined with other data, Sphinx can be a great way to get users fast,
meaningful results. The one downside with using a document based search engine
is that there is little room for returning completely tailored results. Unlike
MySQL, which allows you to slice and dice data in any way you choose, it is
more difficult to return results that take into account relationship specific
to users and documents. However, for most search tasks, it should function
very well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Out of the Terminal]]></title>
    <link href="http://chr.ishenry.com/2010/07/21/out-of-the-terminal/"/>
    <updated>2010-07-21T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2010/07/21/out-of-the-terminal</id>
    <content type="html"><![CDATA[<p>Every once and a while I get to leave server-land and get to do some fun
projects that involve doing something on the front end. The latest was
building an embed script for the Behance <a href="http://www.behance.net/joblist">Job
List</a>. Projects like this, that get me out of
the terminal and into a space that requires a bit more interaction between
domains, are particularly appealing. As much as I think the <a href="http://en.wikipedia.org/wiki/Same_origin_policy">Same Origin
Policy</a> is reasonable rule
for security, I love looking at ways to get around it.</p>

<p>The technique I chose for this was
<a href="http://en.wikipedia.org/wiki/JSONP#JSONP">JSONP</a>, or JSON with Padding. I'm a
huge fan of JSON as a transport, as I feel it is compact, flexible and
stupidly simple to generate and consume. In fact, I've sworn to never touch
another XML file as long as I live. JSONP is really convenient from a API
implementation perspective, because when the request for the data is made (via
the script tag), all the client has to do is pass a callback and it can use
the data in any way it chooses. The server doesn't have to be aware of what
the callback actually does, although I do recommend checking against a list of
pre-approved callbacks, just to make sure.</p>

<p>Like any semi-decent developer, I have dog-fooded my own work, and implemented
the Behance Joblist embed code right <a href="/design-jobs/">here.</a></p>

<p>A little about the Behance Joblist:</p>

<blockquote><p>Top global companies find and hire talent on Behance, the world's leading
network for creative professionals.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sphinx Full Text Search Engine]]></title>
    <link href="http://chr.ishenry.com/2010/01/29/sphinx-full-text-search-engine/"/>
    <updated>2010-01-29T00:00:00-05:00</updated>
    <id>http://chr.ishenry.com/2010/01/29/sphinx-full-text-search-engine</id>
    <content type="html"><![CDATA[<p>For a very long time, I was convinced that a FULLTEXT index in MySQL was the
best solution for all your searching needs. Then I realized that it was
horribly slow, and mixing with complex joins completely destroyed any chances
of using MySQL indexes in any way that would make sense or get decent results.
The solution to fast and scalable free text search on any website is, of
course, a Full Text search engine.</p>

<p>There are a few different ones out there. After a brief affair with Lucene, I
settled on Sphinx. Sphinx is easy to install, even on 64-bit machines, and is
architected in a way that makes a lot of sense for the web. The following
steps were performed on a Red Hat machine. Don't skip the mysql-dev install,
even if you already MySQL installed.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Sphinx installation  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>yum install gcc-c++
</span><span class='line'>yum install mysql-dev*
</span><span class='line'>wget http://www.sphinxsearch.com/downloads/sphinx-0.9.9.tar.gz
</span><span class='line'>tar xzvf sphinx-0.9.9.tar.gz
</span><span class='line'>mkdir /usr/local/sphinx
</span><span class='line'>./configure --prefix /usr/local/sphinx --with-mysql&gt; ./make
</span><span class='line'>make install
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Once installed, it's fairly simple to start playing with the packaged example
data and queries. The php APIs make integration easy, either to build a
service, or use locally as a substitute for MySQL. In fact, as long as the
index can be kept reasonably up to date, Sphinx is a better choice for
complicated sorts than MySQL.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fix or Manage?]]></title>
    <link href="http://chr.ishenry.com/2009/10/24/fix-or-manage/"/>
    <updated>2009-10-24T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2009/10/24/fix-or-manage</id>
    <content type="html"><![CDATA[<p>Sometimes bugs come along that require significant work to fix. Depending on
what project timelines are like at the moment, sometimes fixing the bug isn't
the best option. For example, a race condition in the caching architecture
causes pages to be stale. The persistent data store is correct, but the cache
is not. To the person who just triggered the update, there's a bug. The
information on the public side is not in sync with the information they just
entered.</p>

<p>So, like any other bug, a report will eventually percolate down to the dev
team. People scream, fortunes are lost, the svn blame command is used, and the
devs who wrote the code pee their pants. Once the chaos dies down, the actual
prognosis of this issue can turn out to be extremely grim.</p>

<p>A shortcoming of the caching architecture shows that there's a race condition
when the system is under heavy load. In order to fix it, the dev team needs to
plumb the depths of the data access layer, and probably change some
parameters. But that'll probably break everything. Everywhere. Or the layer
manipulating the data could be fixed to replace the cache instead of
invalidating. Except the methods to manipulate that entity live in 3 different
codebases. It'll probably break the editor. Either way, the actual solution
doesn't matter.The dev team certainly needs to do something, and it needs to
be released three days ago.</p>

<p>The correct way to fix this issue will vary widely depending on circumstances.
But in this particular case, the best answer was to <strong>not fix it, just manage
it</strong>. Our team was busy, there were other projects that were more pressing.
Plus the codebase was being rewritten. So instead of flogging a dead horse, a
simple script was thrown together that compared the cache and the database. If
they were out of sync, the cache would be cleared, and would be repopulated
with the correct information the next time it was requested. Once it was
implemented, the bug was still there, but the cache <em>seemed</em> to be up to date.</p>

<p>Every dev team will face bugs that have enormous costs to fix. The way to deal
with these bugs will be different every time they come up. It's important to
remember that managing bugs can be almost as effective as fixing them.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Do one thing, but do it really well]]></title>
    <link href="http://chr.ishenry.com/2009/07/08/do-one-thing-but-do-it-really-well/"/>
    <updated>2009-07-08T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2009/07/08/do-one-thing-but-do-it-really-well</id>
    <content type="html"><![CDATA[<p>In life there are people who will consider themselves Jacks of all Trades. But
as the saying goes, they are master of none.</p>

<p>Websites will always start out as a codebase that does everything. There will
be a couple files that add users, encode video to Flash, pull rss feeds,
assemble HTML, update products, charge users, manipulate images, redirect old
links, handle file uploads, calculate shipping, delete categories, create rss
feeds, search the database, etc. Sometimes the code to do these will be
organized into files, sometimes it won't. The whole site will run on a single
server, or more likely, a slice of a single server.</p>

<p>None of the things in the above list will be done well. None. This is mostly
because there is too little code and too little hardware focused on doing too
much. Also, every piece of code will be tightly coupled. So any one of those
features could potentially get a ton of traffic, or hit a bump, and consume a
ton of resources. Once that happens, it's safe to assume the whole thing will
go down in flames.</p>

<p>So to avoid the Fail Whale, its really important to build sites as a group of
components that work together. <a href="http://bit.ly/TunGR">Architecture</a> is key, and
when carefully thought out, can ensure that the most important parts of the
site stay up. Even when your image manipulation script on the backend freaks
out, the home page should continue to load flawlessly.</p>

<p>With database-driven apps (almost every major site on the web), there needs to
be particular attention paid to a caching layer. Again, since most sites start
out with a jumbled codebase, the likelihood that all the code to manage data
is in the same place is unlikely. Given the complexities of managing cache
objects, making sure that objects are invalidated on update is crucial to
making updates look seamless. So there needs to be a set of code that's good
at one thing: managing data and its cache.</p>

<p>Search is another area that commonly relies on database, and can eat a ton of
resources. If performing search in SQL, difficult queries can lock tables and
keep other queries from being answered. As good as some DBMSes have gotten at
handling search ( ie MySQL's FULLTEXT ), they still can't fulfill the
concurrency demands of a site with heavy traffic. So, again, the solution is a
change where a resource intensice feature needs to be isolated from other
code. There are a few different ways to do this. One is running replication,
which may not be possible in smaller hosting environments. Another is to use
Full Text Search (Lucene, Sphinx, etc.) Again, this may not be possible in
smaller hosting environments.</p>

<p>Using code that's already good at managing and retrieving data, an interface
can be built to query your data. A second hosting environment that's suitable
for running the search tool of your choice can then query the data code for
updates it needs to keep itself updated. In turn, this server will return
search results without tying up any resources necessary for doing important
stuff, like serving the home page.</p>

<p>So in these two short examples, we've created a theoretical architecture that
can sustain heavy, site-breaking traffic to the search, and still continue to
serve the home page. Of course, until the Apache server becomes so inundated
with requests that it can't do anything. Then it's time to get that load
balancer in place…</p>
]]></content>
  </entry>
  
</feed>
