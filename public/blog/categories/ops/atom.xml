<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ops | The Technician]]></title>
  <link href="http://chr.ishenry.com/blog/categories/ops/atom.xml" rel="self"/>
  <link href="http://chr.ishenry.com/"/>
  <updated>2013-06-30T22:01:36-04:00</updated>
  <id>http://chr.ishenry.com/</id>
  <author>
    <name><![CDATA[Chris Henry]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MySQL's INSERT SELECT, Replication, and You]]></title>
    <link href="http://chr.ishenry.com/2012/08/13/mysqls-insert-select-replication-and-you/"/>
    <updated>2012-08-13T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2012/08/13/mysqls-insert-select-replication-and-you</id>
    <content type="html"><![CDATA[<p>Whenever there are situations where data needs to be copied from table to
table, or SELECTing lots of rows to be inserted, the <a href="http://dev.mysql.com/doc/refman/5.5/en/insert-select.html">INSERT
SELECT</a> is an
elegant solution. It reduces the number of queries sent to a MySQL server, and
makes for elegant code. Additionally, with INSERT SELECTs, all processing
happens on the MySQL side. The app doesn't have to deal with having any of the
data in memory. This means that application servers can be run with less
memory.</p>

<p>Unfortunately, INSERT SELECT's best use cases coincide with cases where the
SELECT query has the potential to run a long time. On standalone servers
running InnoDB, this can be fine, as reads and writes will continue to execute
concurrently. However, if you're running MyISAM, queries will lock, and
nothing will execute. Instead, queries will queue up, your application will
come to a dead halt, MySQL will likely hit max_connections and Very Bad Things
will happen.</p>

<p>In <a href="http://dev.mysql.com/doc/refman/5.5/en/replication-%0Aimplementation.html">replicated</a> environments, even well tuned ones running on InnoDB, a
long running INSERT SELECT can cause other sorts of problem. MySQL replication
is statement based. In other words, every statement that writes to disk on the
master is written to a log. The log is then transferred to slave(s), and those
statements are replayed on the slaves.</p>

<p>With INSERT SELECTS, every slave needs to run the same SELECT. The master will
not simply pass on the results of the SELECT, but rather simply pass the same
query to be executed by the slave(s). So in a replicated environment, it's
even more important to keep an eye on how long those INSERT SELECTS are
running. Not only is it a waste of processing power to run the SELECT portion
of the query across the entire cluster, the SELECT will actually block other
statements in the log from executing. This means that the data on the slaves
will become out of sync with the master, a condition called slave lag.</p>

<p>INSERT SELECT is a great tool, but beware of the costs of using it in certain
situations.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Distributed Updates]]></title>
    <link href="http://chr.ishenry.com/2011/06/25/distributed-updates/"/>
    <updated>2011-06-25T00:00:00-04:00</updated>
    <id>http://chr.ishenry.com/2011/06/25/distributed-updates</id>
    <content type="html"><![CDATA[<p>Part of managing any large site involves writing scripts that will go through
oyur data, make changes, merge things, remove things, do type transformations,
etc. Most of the time, in PHP, iterating through rows or objects will do just
fine. However, when there are lots of rows or objects, you could be faced with
a script that takes hours or days to run. Depending on how often active the
is, you may need to restrict access to ensure that the data before and after
the transformation remains consistent. In other words, if someone tries to
make a change to the data before the transformation, and the new feature only
looks at data after the transformation, that user has just lost their changes.
That is Very Bad.</p>

<p>As sites get larger and problems like this loom, taking the site offline
becomes less and less of an option. This is what the business team calls a
luxury problem, and what the ops team refers to simply as a problem. One
option is to write a more efficient script. You can get pretty far by simply
ensuring you're reading from the fastest data source available, make good use
of cache, etc. ensure that the tables being read for the transformation are
properly indexed. All of these are great places to start. Additionally, making
sure that data is grabbed in chunks can give the database time to breathe.
There's nothing worse than getting stuck in MySQL's "sending data" phase
simply because it needs to read several thousand rows from disk. MySQL
configuration can also be your friend here. If using InnoDB, increasing the
insert buffer is a great way to speed up writes.*</p>

<p>However, as much as you can do to speed up a single transaction, the fact
remains that you have to execute each transformation serially, one after
another. Your bottleneck is the transformation itself. It will take (# of
transformations * # of objects to transform) to complete the job. No matter
how well tuned the database is, it will only be performing one operation at a
time, which means that the other (max connections - 1) connections are doing
precisely crap. So the next logical step is to change your update script to
distribute the update operations so a few can be run in parrallel.</p>

<p>Rewriting the update script does require thinking about your update
differently, and will not work in every case. For example, if one is simply
moving a large amount of data from one table to another, and there is no
transformation, or the transformation can be accomplished via a builtin MySQL
function, use that. However, just be prepared to deal with locking issues, and
the source data potentially not being available while the transformation is
taking place. However, if your transformation is complicated, and requires
per-case logic, this is definitely a good route to take. The biggest
difference is how the code for the update is organized. The update script
needs to be separated out into code that will apply the transformation for
exactly one entity, and code that will manage which entities get transformed
and when. Ideally, the code for the transformation is idempotent, so failures
can be handled by simply resubmitting the entity / object to be transformed
again.</p>

<p>Accomplishing parallel processing in PHP can be kind of tricky. Php's
pcntl_exec function has always felt a bit finicky to me. Of course exec on its
own it blocking, so that's out. Additionally, neither of these solutions offer
any sort of baked-in communication between the process that submitted the job,
and the process carrying out the job. That leaves us with a queuing system.
Popular systems include: RabbitMQ and Gearman. Personally, I've made great use
of <a href="http://chr.ishenry.com/2009/07/25/gearman/">Gearman</a>. It's easy to
install, as is the PHP module.</p>

<p>To sum up, performing large data updates via a distributed system is the way
to go if you have complex requirements per transformation, and the option to
perform these processes in parallel.</p>

<p>*If using MySQL's MyISAM engine, this isn't necessarily true, as writes will block, and the database could become the bottleneck. However, since MySQL is continuing to push InnnDB, this is getting increasingly unlikely. So if your tables are all InnoDB, you're probably in good shape.</p>
]]></content>
  </entry>
  
</feed>
